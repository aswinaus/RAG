{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/RAG/blob/main/Lyft10K_of_rag_pipeline_pymupdf_langsmith_observability_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXi4vN8Xz3ab"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huxDjqeWyGFU"
      },
      "outputs": [],
      "source": [
        "!pip install pdfminer.six langsmith langchain langchain_openai chromadb pypdf nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "2XX889XkwvZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zZEr0TKMZHpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZGV0yQazm2m"
      },
      "outputs": [],
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    return extract_text(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Data\n",
        "data_dir = '/content/drive/MyDrive' # Input a data dir path from your mounted Google Drive"
      ],
      "metadata": {
        "id": "c7TxMGVaZVwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1: upload a PDF to the root of the file browser\n",
        "# then change the variable below to be the name of your file\n",
        "file_name = 'uber_10k_2023'"
      ],
      "metadata": {
        "id": "kWSGCX3XV1S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_text = extract_text_from_pdf(f\"{data_dir}/RAG/data/10k/uber_10k_2023.pdf\")"
      ],
      "metadata": {
        "id": "bZPr4SpnTtv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_text.split('\\n')[0:10000]"
      ],
      "metadata": {
        "id": "x-1ewZXmUWNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain-community\n"
      ],
      "metadata": {
        "id": "lLYNy_cVc_tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb openai"
      ],
      "metadata": {
        "id": "IrMeiAHIeCQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from langchain.embeddings import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "6CMqEHHQeRZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "loader = PyPDFLoader(f\"{data_dir}/RAG/data/10k/uber_10k_2023.pdf\")\n",
        "# load_and_split uses RecursiveCharacterTextSplitter by default, but here I customize the chunk size & overlap\n",
        "pages = loader.load_and_split(text_splitter)"
      ],
      "metadata": {
        "id": "IJ03AtpqMTfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "hIptf7tPgJqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create vector store with Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.vectorstores.utils import filter_complex_metadata # import filter_complex_metadata\n",
        "\n",
        "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "# filtered_chunked_markdown = filter_complex_metadata(chunked_markdown)\n",
        "\n",
        "#index = Chroma.from_documents(documents=pages, embedding=OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"]))\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=pages, embedding=OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"]),persist_directory=f\"{data_dir}/RAG/VectorDB/chroma_db_RAG_FOR_Evals\")\n",
        "vectordb.persist()\n",
        "retriever = vectordb.as_retriever()"
      ],
      "metadata": {
        "id": "SKiEmXKUpkw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multi-query\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"You are an AI language model Accounting assistant. Your task is to generate five\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_perspectives\n",
        "    | ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ],
      "metadata": {
        "id": "97K0pYTM8Vbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questiononRevenue = \"Please summarize Financial and Operational Highlights for Uber?\""
      ],
      "metadata": {
        "id": "SO4SPXTqbrra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_queries.invoke(questiononRevenue)"
      ],
      "metadata": {
        "id": "dbtMR8sdbw3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Could you give me a concise overview of the Business strategies employed by Uber?\""
      ],
      "metadata": {
        "id": "nTcBtmM0Naox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_queries.invoke(question)"
      ],
      "metadata": {
        "id": "wheaWAzFAlCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve docs given a list of queries\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "#retriever = MultiQueryRetriever.from_llm(\n",
        "#    retriever=index.as_retriever(), llm=llm\n",
        "#)"
      ],
      "metadata": {
        "id": "bXJGMOQGvBj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rank documents\n",
        "from langchain.load import dumps, loads\n",
        "\n",
        "def rank_documents(results: list[list], k=60):\n",
        "    fused_scores = {}\n",
        "\n",
        "    for docs in results:\n",
        "        for rank, doc in enumerate(docs):\n",
        "            doc_str = dumps(doc)\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    return reranked_results\n",
        "\n",
        "retrieval_chain = generate_queries | retriever.map() | rank_documents\n",
        "docs = retrieval_chain.invoke(questiononRevenue)"
      ],
      "metadata": {
        "id": "yhPQwRDdGRsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "_ZjOjGUhxDAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a RAG Pipeline\n",
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "# Chain\n",
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_chain, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"question\":question})"
      ],
      "metadata": {
        "id": "PqRt164pUbYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGSMITH_TRACING\"]=\"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"]=userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGSMITH_PROJECT\"]=\"rag_observability\"\n",
        "os.environ[\"OPENAI_API_KEY\"]=userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "qnbeNXz_Z9kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(f\"LANGSMITH_TRACING: {os.environ.get('LANGSMITH_TRACING')}\")\n",
        "print(f\"LANGSMITH_ENDPOINT: {os.environ.get('LANGSMITH_ENDPOINT')}\")\n",
        "print(f\"LANGSMITH_API_KEY: {os.environ.get('LANGSMITH_API_KEY')}\")\n",
        "print(f\"LANGSMITH_PROJECT: {os.environ.get('LANGSMITH_PROJECT')}\")"
      ],
      "metadata": {
        "id": "gu_7tDjac_hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LANGSMITH_TRACING=True\n",
        "LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n",
        "LANGSMITH_API_KEY=userdata.get('LANGCHAIN_API_KEY')\n",
        "LANGSMITH_PROJECT=\"rag_observability\"\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "Pb4mQBo4gHug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client, traceable\n",
        "import openai\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "\n",
        "client = Client(api_key=LANGSMITH_API_KEY)\n",
        "\n",
        "#client = Client() # replace with your api key if you don't have environment variables set\n",
        "\n",
        "#@traceable(run_type=\"chain\")  # or client=client, if you don't have environment variables set\n",
        "@traceable(run_type=\"chain\", client=client, tracing_level=\"verbose\")\n",
        "def run_llm(prompt: str):\n",
        "    llm = OpenAI()  # or any other llm you want to test with\n",
        "    return llm(prompt)\n",
        "\n",
        "run_llm(\"what is the meaning of life in the context of an IT Engineer and how will that change in the AI world in future?\")"
      ],
      "metadata": {
        "id": "OLtdKouXfg_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from langsmith.wrappers import wrap_openai\n",
        "from langsmith import traceable\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain.load import dumps, loads\n",
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Auto-trace LLM calls in-context\n",
        "#client = wrap_openai(openai.Client())\n",
        "client = Client(api_key=LANGSMITH_API_KEY)\n",
        "@traceable # Auto-trace this function\n",
        "def pipeline(user_input: str):\n",
        "    # --- final_rag_chain logic starts here ---\n",
        "\n",
        "    # generate_queries from final_rag_chain\n",
        "    template = \"\"\"You are an AI language model Accounting assistant. Your task is to generate five\n",
        "    different versions of the given user question to retrieve relevant documents from a vector\n",
        "    database. By generating multiple perspectives on the user question, your goal is to help\n",
        "    the user overcome some of the limitations of the distance-based similarity search.\n",
        "    Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "    prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "    openai_api_key = os.environ[\"OPENAI_API_KEY\"]  # Assuming you have OPENAI_API_KEY set\n",
        "\n",
        "    generate_queries = (\n",
        "        prompt_perspectives\n",
        "        | ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "        | StrOutputParser()\n",
        "        | (lambda x: x.split(\"\\n\"))\n",
        "    )\n",
        "\n",
        "    # retriever from final_rag_chain (assuming you have 'vectordb' defined)\n",
        "    retriever = vectordb.as_retriever()\n",
        "\n",
        "    # rank_documents from final_rag_chain\n",
        "    def rank_documents(results: list[list], k=60):\n",
        "        fused_scores = {}\n",
        "        for docs in results:\n",
        "            for rank, doc in enumerate(docs):\n",
        "                doc_str = dumps(doc)\n",
        "                if doc_str not in fused_scores:\n",
        "                    fused_scores[doc_str] = 0\n",
        "                previous_score = fused_scores[doc_str]\n",
        "                fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "        reranked_results = [\n",
        "            (loads(doc), score)\n",
        "            for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "        ]\n",
        "        return reranked_results\n",
        "\n",
        "    retrieval_chain = generate_queries | retriever.map() | rank_documents\n",
        "\n",
        "    # RAG prompt from final_rag_chain\n",
        "    template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "    llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "    # final_rag_chain assembled\n",
        "    final_rag_chain = (\n",
        "        {\"context\": retrieval_chain, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    # --- final_rag_chain logic ends here ---\n",
        "\n",
        "    # Invoke final_rag_chain with user_input\n",
        "    response = final_rag_chain.invoke({\"question\": user_input})\n",
        "\n",
        "    # Log the response and potentially other metadata\n",
        "    # ... (your logging logic) ...\n",
        "\n",
        "    return response\n",
        "\n",
        "pipeline(\"What are the key business strategies utilized by Uber?\")"
      ],
      "metadata": {
        "id": "YpYeOtGxXdsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"Uber_10K_2023_v6\""
      ],
      "metadata": {
        "id": "vBmLhupyz0B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataset\n",
        "from langsmith import Client\n",
        "import os\n",
        "\n",
        "dataset_inputs = [\n",
        "    '1. What are the key business strategies utilized by Uber?',\n",
        " '2. Can you provide a summary of Ubers business strategies?',\n",
        " '3. How does Uber approach its business strategies?',\n",
        " '4. What are the main tactics Uber uses to drive its business success?',\n",
        " '5. Could you outline the strategic approaches taken by Uber in its operations?'\n",
        "]\n",
        "\n",
        "dataset_outputs = [\n",
        "    {\"must_mention\": [\"ee the section titled “Reconciliations of Non-GAAP Financial Measures\", \"“Certain Key Metrics and Non-GAAP Financial Measures” below for more information\", \"MAPCs presented for annual periods are MAPCs for the fourth quarter of the year.\"]},\n",
        "    {\"must_mention\": [\"the following discussion contains forward-looking statements that reflect our plans, estimates,and beliefs. Our actual results could differ materially from those discussed in the forward-looking statements.\", '“Risk Factors”, for a discussion of factors thatcould cause actual results to differ materially from the results described in or implied by the forward-looking statements contained in the following discussion andanalysis']},\n",
        "    {\"must_mention\": [\"massive network, leading technology, operational excellence, and product expertise to power movement from point Ato point B. We develop and operate proprietary technology applications supporting a variety of offerings on our platform. We connect consumers with providers ofride services, merchants as well as delivery service providers for meal preparation, grocery and other delivery services.\"]},\n",
        "    {\"must_mention\": [\"Revenue $ 31,877 $ 37,281 17 % 18 %\\nIncome (loss) from operations $ (1,832) $ 1,110 **\\nNet income (loss) attributable to Uber Technologies, Inc. $ (9,141) $ 1,887\"]}, # reading from a table\n",
        "    {\"must_mention\": [\"EBITDA $ 1,713 $ 4,052 137 %\\nNet cash provided by operating activities $ 642 $ 3,585 **\\nFree cash flow $ 390 $ 3,362 **\\n See the section titled “Reconciliations of Non-GAAP Financial Measures” for more information and reconciliations to the most directly comparable GAAPfinancial measure.\\n See the section titled “Certain Key Metrics and Non-GAAP Financial Measures” below for more information.\\n MAPCs presented for annual periods are MAPCs for the fourth quarter of the year.\"]}, # reading from a table\n",
        "]\n",
        "\n",
        "# ensure you have this set up\n",
        "from google.colab import userdata\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "langchain_api_key = os.environ[\"LANGCHAIN_API_KEY\"]\n",
        "\n",
        "client = Client(api_key=langchain_api_key)\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Uber 10K 2023 questions\",\n",
        ")\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\": q} for q in dataset_inputs],\n",
        "    outputs=dataset_outputs,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ],
      "metadata": {
        "id": "jZiPRupZojqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooYcQV24j-wD"
      },
      "outputs": [],
      "source": [
        " #run evals in langsmith\n",
        "from langsmith.schemas import Run, Example\n",
        "\n",
        "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
        "\n",
        "\n",
        "def must_mention(run: Run, example: Example) -> dict:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = any(phrase in prediction for phrase in required)\n",
        "    return {\"key\":\"must_mention\", \"score\": score}\n",
        "\n",
        "evaluators = [\n",
        "  must_mention,\n",
        "]\n",
        "runner = final_rag_chain\n",
        "def query_wrapper(query_dict):\n",
        "    query_string = query_dict['question']\n",
        "    response = runner.invoke(query_string)\n",
        "    return {\"output\": response}\n",
        "\n",
        "experiment_results = evaluate(\n",
        "    query_wrapper,\n",
        "    data=dataset_name,\n",
        "    evaluators=evaluators,\n",
        "    experiment_prefix=\"uber10Kv1\",\n",
        "    client=client,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-openai"
      ],
      "metadata": {
        "id": "wmWTVN3CivKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "llm.invoke(\"Hello, world!\")"
      ],
      "metadata": {
        "id": "u5_e9v_GkMcY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}