{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/RAG/blob/main/Lyft10K_of_rag_pipeline_pymupdf_langsmith_article.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXi4vN8Xz3ab"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huxDjqeWyGFU"
      },
      "outputs": [],
      "source": [
        "!pip install pdfminer.six langsmith langchain langchain_openai chromadb pypdf nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "2XX889XkwvZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zZEr0TKMZHpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZGV0yQazm2m"
      },
      "outputs": [],
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    return extract_text(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Data\n",
        "data_dir = '/content/drive/MyDrive' # Input a data dir path from your mounted Google Drive"
      ],
      "metadata": {
        "id": "c7TxMGVaZVwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1: upload a PDF to the root of the file browser\n",
        "# then change the variable below to be the name of your file\n",
        "file_name = 'uber_10k_2023'"
      ],
      "metadata": {
        "id": "kWSGCX3XV1S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_text = extract_text_from_pdf(f\"{data_dir}/RAG/data/10k/uber_10k_2023.pdf\")"
      ],
      "metadata": {
        "id": "bZPr4SpnTtv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_text.split('\\n')[0:10000]"
      ],
      "metadata": {
        "id": "x-1ewZXmUWNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain-community\n"
      ],
      "metadata": {
        "id": "lLYNy_cVc_tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb openai"
      ],
      "metadata": {
        "id": "IrMeiAHIeCQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from langchain.embeddings import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "6CMqEHHQeRZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "loader = PyPDFLoader(f\"{data_dir}/RAG/data/10k/uber_10k_2023.pdf\")\n",
        "# load_and_split uses RecursiveCharacterTextSplitter by default, but here I customize the chunk size & overlap\n",
        "pages = loader.load_and_split(text_splitter)"
      ],
      "metadata": {
        "id": "IJ03AtpqMTfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "hIptf7tPgJqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create vector store with Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.vectorstores.utils import filter_complex_metadata # import filter_complex_metadata\n",
        "\n",
        "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "# filtered_chunked_markdown = filter_complex_metadata(chunked_markdown)\n",
        "\n",
        "#index = Chroma.from_documents(documents=pages, embedding=OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"]))\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=pages, embedding=OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"]),persist_directory=f\"{data_dir}/RAG/VectorDB/chroma_db_RAG_FOR_Evals\")\n",
        "vectordb.persist()\n",
        "retriever = vectordb.as_retriever()"
      ],
      "metadata": {
        "id": "SKiEmXKUpkw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multi-query\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"You are an AI language model Accounting assistant. Your task is to generate five\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_perspectives\n",
        "    | ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ],
      "metadata": {
        "id": "97K0pYTM8Vbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questiononRevenue = \"Please summarize Financial and Operational Highlights for Uber?\""
      ],
      "metadata": {
        "id": "SO4SPXTqbrra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_queries.invoke(questiononRevenue)"
      ],
      "metadata": {
        "id": "dbtMR8sdbw3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Does Uber have the liquidity to meet its working capital and capital expenditures needs.Please explain?\""
      ],
      "metadata": {
        "id": "nTcBtmM0Naox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_queries.invoke(question)"
      ],
      "metadata": {
        "id": "wheaWAzFAlCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve docs given a list of queries\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "#retriever = MultiQueryRetriever.from_llm(\n",
        "#    retriever=index.as_retriever(), llm=llm\n",
        "#)"
      ],
      "metadata": {
        "id": "bXJGMOQGvBj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rank documents\n",
        "from langchain.load import dumps, loads\n",
        "\n",
        "def rank_documents(results: list[list], k=60):\n",
        "    fused_scores = {}\n",
        "\n",
        "    for docs in results:\n",
        "        for rank, doc in enumerate(docs):\n",
        "            doc_str = dumps(doc)\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    return reranked_results\n",
        "\n",
        "retrieval_chain = generate_queries | retriever.map() | rank_documents\n",
        "docs = retrieval_chain.invoke(questiononRevenue)"
      ],
      "metadata": {
        "id": "yhPQwRDdGRsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "_ZjOjGUhxDAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a RAG Pipeline\n",
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "# Chain\n",
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_chain, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"question\":question})"
      ],
      "metadata": {
        "id": "PqRt164pUbYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"Uber_10K_2023_v2\""
      ],
      "metadata": {
        "id": "vBmLhupyz0B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataset\n",
        "from langsmith import Client\n",
        "import os\n",
        "\n",
        "dataset_inputs = [\n",
        "    '1. Can Uber cover its operational and investment expenses with its available funds?',\n",
        " '2. Is Uber financially equipped to handle its day-to-day expenses and long-term investments?',\n",
        " '3. Does Uber possess enough financial resources to support its working capital and capital expenditure requirements?',\n",
        " '4. How well is Uber positioned to meet its financial obligations for both short-term and long-term needs?',\n",
        " '5. Is Uber adequately funded to address its working capital and capital expenditure demands effectively?'\n",
        "]\n",
        "\n",
        "dataset_outputs = [\n",
        "    {\"must_mention\": [\"Uber's total assets were $38,699 million\", \"Additionally, Uber had restricted cash and cash equivalents of $805 million\", \"The company's financial position suggests that it has the resources to cover its short-term obligations and fund its operational requirements.\", \"1\"]},\n",
        "    {\"must_mention\": [\"$12,682 million ($38,699 million - $26,017 million)\", 'global company and as of December 31, 2023, we and our subsidiaries had approximately 30,400 employees globally and operations inapproximately 70 countries and more than 10,000 cities around the world', '$12,682 million ']},\n",
        "    {\"must_mention\": [\"$5,407 million in liquid assets\", 'Freight Gross Bookings declining 25% year-over-year.']}, # reading from a table\n",
        "    {\"must_mention\": [\"The company's financial position suggests that it has the resources to cover its short-term obligations and fund its operational requirements.\"]}, # reading from a table\n",
        "    {\"must_mention\": [\"Uber had cash and cash equivalents of $4,680 million and short-term investments of $727 million\"]}, # reading from a table\n",
        "]\n",
        "\n",
        "# ensure you have this set up\n",
        "from google.colab import userdata\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "langchain_api_key = os.environ[\"LANGCHAIN_API_KEY\"]\n",
        "\n",
        "client = Client(api_key=langchain_api_key)\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Uber 10K 2023 questions\",\n",
        ")\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\": q} for q in dataset_inputs],\n",
        "    outputs=dataset_outputs,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ],
      "metadata": {
        "id": "jZiPRupZojqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooYcQV24j-wD"
      },
      "outputs": [],
      "source": [
        " #run evals in langsmith\n",
        "from langsmith.schemas import Run, Example\n",
        "\n",
        "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
        "\n",
        "\n",
        "def must_mention(run: Run, example: Example) -> dict:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = any(phrase in prediction for phrase in required)\n",
        "    return {\"key\":\"must_mention\", \"score\": score}\n",
        "\n",
        "evaluators = [\n",
        "  must_mention,\n",
        "]\n",
        "runner = final_rag_chain\n",
        "def query_wrapper(query_dict):\n",
        "    query_string = query_dict['question']\n",
        "    response = runner.invoke(query_string)\n",
        "    return {\"output\": response}\n",
        "\n",
        "experiment_results = evaluate(\n",
        "    query_wrapper,\n",
        "    data=dataset_name,\n",
        "    evaluators=evaluators,\n",
        "    experiment_prefix=\"uber10Kv1\",\n",
        "    client=client,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-openai"
      ],
      "metadata": {
        "id": "wmWTVN3CivKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "llm.invoke(\"Hello, world!\")"
      ],
      "metadata": {
        "id": "u5_e9v_GkMcY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}