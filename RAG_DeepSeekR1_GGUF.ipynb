{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1ji8P6NnoigiwFZXxJBTo0lfKnj5QBOYj",
      "authorship_tag": "ABX9TyMhWxa+Ur1GbmlbyPSWYHT5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/RAG/blob/main/RAG_DeepSeekR1_GGUF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbxi59eeGs7b"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers torch accelerate bitsandbytes langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code is essentially forcing Python to always use \"UTF-8\" as the preferred encoding, regardless of the user's actual system settings. UTF-8 is a widely used encoding that can represent a vast range of characters from different languages. By enforcing UTF-8, you can help ensure that your code works consistently across different platforms and avoids encoding-related errors. It's a common practice for improving compatibility and preventing issues with text handling in Python programs."
      ],
      "metadata": {
        "id": "vpCeli-FI8me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "c5-4gG8tIrFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pyngrok is a Python wrapper for ngrok, a tool that allows you to expose a local web server to the public internet. This can be very useful for sharing your work, testing webhooks, or building demos that need to be accessible from the outside.\n",
        "!pip install pyngrok --quiet\n",
        "#This is a modern, fast (high-performance) web framework for building APIs with Python 3.6+ based on standard Python type hints. It's often used for creating web applications and services.\n",
        "!pip install fastapi nest-asyncio --quiet\n",
        "#uvicorn is an ASGI (Asynchronous Server Gateway Interface) web server. This essentially means that it's a tool that can run Python web applications designed for asynchronous operation and handling many requests concurrently. It's often used with modern Python web frameworks like FastAPI to serve the application to users.\n",
        "!pip install uvicorn --quiet\n",
        "!pip install langchain-community --quiet\n",
        "!pip install -U langchain-huggingface --quiet"
      ],
      "metadata": {
        "id": "BvV4bJmTJKSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HUGGING_FACE_TOKEN = userdata.get('HUGGING_FACE_TOKEN')"
      ],
      "metadata": {
        "id": "UGGq_MH0LK3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token $HUGGING_FACE_TOKEN\n"
      ],
      "metadata": {
        "id": "6aIoujSHLcnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer , BitsAndBytesConfig\n",
        "import transformers\n",
        "import torch\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from threading import Thread\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "ML6AIgxOLraR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The nvidia-smi command is a utility provided by NVIDIA to query and display information about your NVIDIA GPU(s) (Graphics Processing Unit). This includes things like:\n",
        "\n",
        "GPU model and name\n",
        "Driver version\n",
        "GPU utilization\n",
        "Memory usage\n",
        "Temperature\n",
        "Power consumption\n",
        "Processes running on the GPU"
      ],
      "metadata": {
        "id": "Im62RU0RMXkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "GHtMY71ZL6tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text(text, width=90): #preserve_newlines\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "58tu7qi6N1ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, Optional, Union, Dict, Any\n",
        "from transformers import PreTrainedModel, AutoModel, AutoTokenizer, AutoConfig\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase"
      ],
      "metadata": {
        "id": "eF2xmVtIp14B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function provides a robust way to load a pre-trained model, prioritizing quantization for optimization but gracefully falling back to a non-quantized version if necessary. This helps ensure compatibility and flexibility when working with different models and environments"
      ],
      "metadata": {
        "id": "GRA4SC7_C-2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_with_quantization_fallback(\n",
        "    model_name: str = \"/content/DeepSeek-R1-GGUF\",\n",
        "    trust_remote_code: bool = True,\n",
        "    **kwargs\n",
        ") -> Tuple[PreTrainedModel, PreTrainedTokenizerBase]:\n",
        "\n",
        "  try:\n",
        "      config = AutoConfig.from_pretrained(\n",
        "              model_name,\n",
        "              trust_remote_code=trust_remote_code\n",
        "          )\n",
        "      config.model_type = \"DeepSeek-R1\"\n",
        "      model = AutoModel.from_pretrained(\n",
        "          model_name,\n",
        "          config=config,\n",
        "          model_type=\"DeepSeek-R1\",\n",
        "          trust_remote_code=trust_remote_code,\n",
        "          device_map=device_map,\n",
        "          **kwargs\n",
        "      )\n",
        "      tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      print(\"Model loaded successfully with original configuration\")\n",
        "      return model, tokenizer\n",
        "  except ValueError as e:\n",
        "      if \"Unknown quantization type\" in str(e):\n",
        "          print(\n",
        "              \"Quantization type not supported directly. \"\n",
        "              \"Attempting to load without quantization...\"\n",
        "          )\n",
        "\n",
        "\n",
        "          if hasattr(config, \"quantization_config\"):\n",
        "              delattr(config, \"quantization_config\")\n",
        "\n",
        "          try:\n",
        "              model = AutoModel.from_pretrained(\n",
        "                  model_name,\n",
        "                  config=config,\n",
        "                  model_type=\"DeepSeek-R1-GGUF\",\n",
        "                  trust_remote_code=trust_remote_code,\n",
        "                  device_map=device_map,\n",
        "                  **kwargs\n",
        "              )\n",
        "              tokenizer = AutoTokenizer.from_pretrained(\n",
        "                  model_name,\n",
        "                  trust_remote_code=trust_remote_code\n",
        "              )\n",
        "              print(\"Model loaded successfully without quantization\")\n",
        "              return model, tokenizer\n",
        "\n",
        "          except Exception as inner_e:\n",
        "              print(f\"Failed to load model without quantization: {str(inner_e)}\")\n",
        "              raise\n",
        "      else:\n",
        "          print(f\"Unexpected error during model loading: {str(e)}\")\n",
        "          raise"
      ],
      "metadata": {
        "id": "Vd4izpgNoAxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "# Determine the device to load the model on (CPU or GPU)\n",
        "device_map = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "!pip install huggingface_hub hf_transfer --quiet\n",
        "# import os # Optional for faster downloading\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(\n",
        "  repo_id = \"unsloth/DeepSeek-R1-GGUF\",\n",
        "  local_dir = \"DeepSeek-R1-GGUF\",\n",
        "  allow_patterns = [\"*UD-IQ1_S*\"], # Select quant type UD-IQ1_S for 1.58bit\n",
        ")\n",
        "#/content/DeepSeek-R1-GGUF\n",
        "import torch\n",
        "# Now you can call the function:\n",
        "model, tokenizer = load_model_with_quantization_fallback()"
      ],
      "metadata": {
        "id": "VqjUsnNurzWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets langchain langchain_community langchain_openai chromadb --quiet"
      ],
      "metadata": {
        "id": "aSAA05IEI8KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "from langchain_core.runnables import (\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough\n",
        ")\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ],
      "metadata": {
        "id": "sTwf3skEJwd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "BaSxtHDEIvEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "dataset = load_dataset(\"aswinaus/tax_statistics_dataset_by_income_range\", download_mode=\"force_redownload\")\n",
        "df=pd.DataFrame(dataset['train'])"
      ],
      "metadata": {
        "id": "uh6PpPiAIyNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Download Data\n",
        "data_dir = '/content/drive/MyDrive'"
      ],
      "metadata": {
        "id": "9VMpzTnAJOuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Any, List\n",
        "from langchain.docstore.document import Document"
      ],
      "metadata": {
        "id": "wVTYVma_8Zzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DatasetDict to LangChain Documents\n",
        "def create_langchain_documents(dataset: Dict[str, Any]) -> List[Document]:\n",
        "    \"\"\"Converts a Hugging Face DatasetDict to a list of LangChain Documents,\n",
        "    including all columns as content.\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    for row in dataset['train']:  # Assuming 'train' is your split name\n",
        "        # Concatenate all column values into a single string\n",
        "        content = \"\\n\".join([f\"{k}: {v}\" for k, v in row.items()])\n",
        "\n",
        "        # Use all columns except 'content' as metadata\n",
        "        metadata = row.copy()\n",
        "\n",
        "        document = Document(page_content=content, metadata=metadata)\n",
        "        documents.append(document)\n",
        "    return documents"
      ],
      "metadata": {
        "id": "dOndvseM8eO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_documents = create_langchain_documents(dataset)\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50,\n",
        ")"
      ],
      "metadata": {
        "id": "vzOoewOF8yrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages = []\n",
        "for document in all_documents:\n",
        "  pages.extend(text_splitter.split_documents([document]))"
      ],
      "metadata": {
        "id": "zL6wa7Ew9FYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create vector store with Chroma\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.vectorstores.utils import filter_complex_metadata # import filter_complex_metadata\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=pages, embedding=OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"]),persist_directory=f\"{data_dir}/RAG/VectorDB/chroma_db_RAG_Income_Tax\")\n",
        "vectordb.persist()\n",
        "retriever = vectordb.as_retriever()"
      ],
      "metadata": {
        "id": "NmzHMoCx9J20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "Vf5oiSrn-uDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_prompt(inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    input_text = inputs[\"prompt\"]\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "    print(f\"Input text: {input_text}\")\n",
        "    print(f\"Input IDs: {input_ids}\")\n",
        "    # Instead of embedding here, return the input_ids directly\n",
        "    #inputs[\"prompt\"] = input_ids  # Replace text with tensor\n",
        "    return input_ids # Return the tensor directly"
      ],
      "metadata": {
        "id": "pmJANV9Pd50S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface --quiet"
      ],
      "metadata": {
        "id": "TSUXVQHwGGzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer , BitsAndBytesConfig\n",
        "import transformers"
      ],
      "metadata": {
        "id": "WiYfePHzJnox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableSequence\n",
        "# RAG\n",
        "template = \"\"\"You are an AI language model Accounting assistant.Answer the following question based on this context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt_creator = ChatPromptTemplate.from_template(template) # moved template to prompt_creator\n"
      ],
      "metadata": {
        "id": "BBkrNzwKgFzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a RAG Pipeline\n",
        "from operator import itemgetter\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableMap\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"You are an AI language model Accounting assistant.Answer the following question based on this context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_creator\n",
        "llm=model\n",
        "final_rag_chain=RunnableSequence(\n",
        "    RunnablePassthrough.assign(\n",
        "        context=lambda x: format_docs(retriever.get_relevant_documents(x[\"question\"])),\n",
        "    )\n",
        "    |{\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | RunnableMap({\"prompt\": encode_prompt})\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "#prompt = ChatPromptTemplate.from_template(template)\n",
        "#llm = model#ChatOpenAI(temperature=0, openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "dOSFPbQR90aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a RAG Pipeline\n",
        "from operator import itemgetter\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableMap\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema import HumanMessage\n",
        "#from langchain.chains import RunnableSequence # Add this import\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"You are an AI language model Accounting assistant.Answer the following question based on this context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_creator\n",
        "llm = model#ChatOpenAI(temperature=0, openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "final_rag_chain = RunnableSequence(\n",
        "    RunnablePassthrough.assign(\n",
        "        context=lambda x: format_docs(retriever.get_relevant_documents(x[\"question\"])),\n",
        "    )\n",
        "    | RunnablePassthrough.assign(\n",
        "        prompt=lambda x: prompt_creator.format(context=x[\"context\"], question=x[\"question\"]) # format template string\n",
        "    ) # This was missing before RunnableMap, leading to KeyError: 'prompt'\n",
        "\n",
        "    | RunnablePassthrough.assign(debug_context=lambda x: print(f\"Context before prompt: {x['context']}\"))\n",
        "    | RunnablePassthrough.assign(debug_question=lambda x: print(f\"Question before prompt: {x['question']}\"))\n",
        "\n",
        "    | RunnablePassthrough.assign(debug_prompt=lambda x: print(f\"Prompt after prompt: {x['prompt']}\"))\n",
        "    # Modified to input the prompt to encode_prompt and extract the tensor\n",
        "    | RunnableMap({\"prompt\": lambda x: encode_prompt({\"prompt\": x['prompt']})})\n",
        "    # Extract the tensor from the dictionary and assign to 'input_ids'\n",
        "    | RunnablePassthrough.assign(input_ids=lambda x: x[\"prompt\"])\n",
        "    # Call the model with the correct arguments (input_ids)\n",
        "    | (lambda x: llm(input_ids=x['input_ids']))\n",
        "    # Decode the output\n",
        "    | (lambda x: tokenizer.decode(x.logits[0], skip_special_tokens=True))\n",
        "    | StrOutputParser()\n",
        "    #Modified line\n",
        "    #| (lambda x: llm.generate(input_ids=x['prompt'], max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id))\n",
        "    #Modified line\n",
        "    #| (lambda x: tokenizer.decode(x[0], skip_special_tokens=True))\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Dq71hpEQYJ4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"What is the number of joint returns for the state of AL for income range $100,000 under $200,000?\"\n"
      ],
      "metadata": {
        "id": "m4_HcaGQ-yR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_rag_chain.invoke({\"question\":question})\n",
        "#final_rag_chain.invoke(question)"
      ],
      "metadata": {
        "id": "7XaNAess-1CB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}