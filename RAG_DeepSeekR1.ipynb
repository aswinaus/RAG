{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMtubiDx2WwdIbJ+uhoLqwx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/RAG/blob/main/RAG_DeepSeekR1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbxi59eeGs7b"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers torch accelerate bitsandbytes langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code is essentially forcing Python to always use \"UTF-8\" as the preferred encoding, regardless of the user's actual system settings. UTF-8 is a widely used encoding that can represent a vast range of characters from different languages. By enforcing UTF-8, you can help ensure that your code works consistently across different platforms and avoids encoding-related errors. It's a common practice for improving compatibility and preventing issues with text handling in Python programs."
      ],
      "metadata": {
        "id": "vpCeli-FI8me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "c5-4gG8tIrFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pyngrok is a Python wrapper for ngrok, a tool that allows you to expose a local web server to the public internet. This can be very useful for sharing your work, testing webhooks, or building demos that need to be accessible from the outside.\n",
        "!pip install pyngrok --quiet\n",
        "#This is a modern, fast (high-performance) web framework for building APIs with Python 3.6+ based on standard Python type hints. It's often used for creating web applications and services.\n",
        "!pip install fastapi nest-asyncio --quiet\n",
        "#uvicorn is an ASGI (Asynchronous Server Gateway Interface) web server. This essentially means that it's a tool that can run Python web applications designed for asynchronous operation and handling many requests concurrently. It's often used with modern Python web frameworks like FastAPI to serve the application to users.\n",
        "!pip install uvicorn --quiet\n",
        "!pip install langchain-community --quiet\n",
        "!pip install -U langchain-huggingface --quiet"
      ],
      "metadata": {
        "id": "BvV4bJmTJKSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HUGGING_FACE_TOKEN = userdata.get('HUGGING_FACE_TOKEN')"
      ],
      "metadata": {
        "id": "UGGq_MH0LK3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token $HUGGING_FACE_TOKEN\n"
      ],
      "metadata": {
        "id": "6aIoujSHLcnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer , BitsAndBytesConfig\n",
        "import transformers\n",
        "import torch\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from threading import Thread\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "ML6AIgxOLraR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The nvidia-smi command is a utility provided by NVIDIA to query and display information about your NVIDIA GPU(s) (Graphics Processing Unit). This includes things like:\n",
        "\n",
        "GPU model and name\n",
        "Driver version\n",
        "GPU utilization\n",
        "Memory usage\n",
        "Temperature\n",
        "Power consumption\n",
        "Processes running on the GPU"
      ],
      "metadata": {
        "id": "Im62RU0RMXkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "GHtMY71ZL6tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text(text, width=90): #preserve_newlines\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "58tu7qi6N1ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, Optional, Union, Dict, Any\n",
        "from transformers import PreTrainedModel, AutoModel, AutoTokenizer, AutoConfig\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase"
      ],
      "metadata": {
        "id": "eF2xmVtIp14B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_with_quantization_fallback(\n",
        "    model_name: str = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "    trust_remote_code: bool = True,\n",
        "    **kwargs\n",
        ") -> Tuple[PreTrainedModel, PreTrainedTokenizerBase]:\n",
        "\n",
        "  try:\n",
        "      model = AutoModel.from_pretrained(\n",
        "          model_name,\n",
        "          trust_remote_code=trust_remote_code,\n",
        "          device_map=device_map,\n",
        "          **kwargs\n",
        "      )\n",
        "      tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      print(\"Model loaded successfully with original configuration\")\n",
        "      return model, tokenizer\n",
        "  except ValueError as e:\n",
        "      if \"Unknown quantization type\" in str(e):\n",
        "          print(\n",
        "              \"Quantization type not supported directly. \"\n",
        "              \"Attempting to load without quantization...\"\n",
        "          )\n",
        "\n",
        "          config = AutoConfig.from_pretrained(\n",
        "              model_name,\n",
        "              trust_remote_code=trust_remote_code\n",
        "          )\n",
        "          if hasattr(config, \"quantization_config\"):\n",
        "              delattr(config, \"quantization_config\")\n",
        "\n",
        "          try:\n",
        "              model = AutoModel.from_pretrained(\n",
        "                  model_name,\n",
        "                  config=config,\n",
        "                  trust_remote_code=trust_remote_code,\n",
        "                  device_map=device_map,\n",
        "                  **kwargs\n",
        "              )\n",
        "              tokenizer = AutoTokenizer.from_pretrained(\n",
        "                  model_name,\n",
        "                  trust_remote_code=trust_remote_code\n",
        "              )\n",
        "              print(\"Model loaded successfully without quantization\")\n",
        "              return model, tokenizer\n",
        "\n",
        "          except Exception as inner_e:\n",
        "              print(f\"Failed to load model without quantization: {str(inner_e)}\")\n",
        "              raise\n",
        "      else:\n",
        "          print(f\"Unexpected error during model loading: {str(e)}\")\n",
        "          raise"
      ],
      "metadata": {
        "id": "Vd4izpgNoAxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Determine the device to load the model on (CPU or GPU)\n",
        "device_map = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Now you can call the function:\n",
        "model, tokenizer = load_model_with_quantization_fallback()"
      ],
      "metadata": {
        "id": "VqjUsnNurzWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets langchain langchain_community langchain_openai chromadb --quiet"
      ],
      "metadata": {
        "id": "aSAA05IEI8KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "from langchain_core.runnables import (\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough\n",
        ")\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ],
      "metadata": {
        "id": "sTwf3skEJwd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "dataset = load_dataset(\"aswinaus/tax_statistics_dataset_by_income_range\", download_mode=\"force_redownload\")\n",
        "df=pd.DataFrame(dataset['train'])"
      ],
      "metadata": {
        "id": "uh6PpPiAIyNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "9VMpzTnAJOuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Any, List\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "T5ukU5xLMeDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DatasetDict to LangChain Documents\n",
        "def create_langchain_documents(dataset: Dict[str, Any]) -> List[Document]:\n",
        "    \"\"\"Converts a Hugging Face DatasetDict to a list of LangChain Documents,\n",
        "    including all columns as content.\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    for row in dataset['train']:  # Assuming 'train' is your split name\n",
        "        # Concatenate all column values into a single string\n",
        "        content = \"\\n\".join([f\"{k}: {v}\" for k, v in row.items()])\n",
        "\n",
        "        # Use all columns except 'content' as metadata\n",
        "        metadata = row.copy()\n",
        "\n",
        "        document = Document(page_content=content, metadata=metadata)\n",
        "        documents.append(document)\n",
        "    return documents"
      ],
      "metadata": {
        "id": "DUH5qnMrMhVe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}